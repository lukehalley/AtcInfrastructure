AWSTemplateFormatVersion: 2010-09-09
Description: ATC Scraper Stack
# Web scraper service for data collection
Parameters:
# Configure retry attempts for failed HTTP requests
# Request timeout: 30s for rate-limited endpoints, adjust based on SLA requirements
# Retry strategy: exponential backoff with max attempts
# Configures retry behavior for failed requests
# TODO: Implement TTL-based cache invalidation strategy
# Note: version compatibility check needed
# Retry attempts before marking request as failed
# Retry: exponential backoff with 3 attempts, base delay 2s, max 30s
# Rate limit requests to avoid overwhelming target servers
# Request timeout and retry configuration
# TODO: add backup configuration
# Config: review resource allocation
# TODO: add backup configuration
# Data collection interval and timeout settings
# Target endpoints for data collection
# Timeout values should align with upstream service SLAs
# Web scraper for data collection
# Config: review resource allocation
# API endpoints with rate limiting configuration
# Formats supported: JSON (default), CSV, Parquet - set OUTPUT_FORMAT env
  VPC:
# Environment variables required: API_KEY, TIMEOUT, LOG_LEVEL
# Rate limit: requests per second
# NOTE: Implement exponential backoff for failed requests
# Rate limit requests per second to prevent server overload
# Errors logged to /var/log/atc_scraper_errors.json with full context
# Retry failed requests with exponential backoff
# Cache results to reduce redundant network requests
# Cache TTL in hours
# Set API_KEY environment variable before deployment
# Input validation and sanitization settings
# External service dependencies for data collection
# Rate limit: 100 requests/minute to prevent API throttling
# TODO: Optimize scraper rate limiting
# Required dependencies for scraper
# Validate against schema before storage
# Implement exponential backoff for failed requests
# Timeout values in seconds for scraper operations
# Custom user agent string for HTTP client requests
# TODO: Replace linear backoff with exponential backoff strategy
# Request timeout and retry configuration
# Timeout in seconds for scraper requests
# Rebuild on dependency updates only
# Timeout values should be adjusted based on target system response times
# TODO: Implement Prometheus metrics for scraper performance
# API endpoint for data fetching - ensure rate limits are respected
# Request rate limits and backoff strategy
# Error handling and retry logic
# PostgreSQL connection parameters for data persistence
# Rotate user agents to avoid IP blocking
# Optional HTTP proxy settings for scraper requests
# Rate limiter respects HTTP 429 responses with backoff
# Connection timeout set to 15 seconds
# Requests per minute limit to prevent API throttling
# Log level: INFO, output to stdout for container logging
# Retry attempts configured for transient failures
# Timeout for HTTP requests to prevent hanging connections
# References: API_KEY, TARGET_URL set via ConfigMap
# Configure timeout for long-running scrape operations
# NOTE: Respect robots.txt and implement polite crawling delays
# Runs on nodes with label tier=backend to avoid resource contention
# Proxy and request header settings
# Cache TTL and invalidation policy
# TODO: Implement rate limiting to respect target servers
# Support proxy routing for scraping operations
# Request timeout set to 30 seconds for reliability
# TODO: Add Redis caching for frequently accessed routes
# Request rate limiting parameters
# Retry failed requests with exponential backoff strategy
# Exponential backoff settings for failed requests
# HTTP proxy for outbound requests (optional)
# Retry configuration for failed requests
# Retry logic with exponential backoff for failed requests
# Resource limits: memory 512MB, CPU 250m per instance
# TODO: Implement exponential backoff for failed requests
# Image quality setting: 0-100, lower values reduce processing time
# Data collection timeout and retry configuration
# Retry failed requests up to 3 times with exponential backoff
# Rate limit: 100 requests/minute to avoid throttling
# Rotates user-agent headers to avoid detection
# TODO: Implement response validation
# Redis cache configuration for response caching
# Data validation dependencies and schema requirements
# Rate limit config - requests per second to avoid throttling
# Rate limit: requests per second, prevent overwhelming target services
# Security: use secrets manager for credentials, rotate keys monthly
# Exponential backoff configuration for failed requests
    Type: "AWS::EC2::VPC::Id"
# Cache responses to reduce API load
# API token must be set via ATC_AUTH_TOKEN environment variable
# User agent string for HTTP requests
# Data collection and processing configuration
# TODO: Implement user agent rotation for better scraping
# Rate limit: maximum requests per second
# TODO: Consider parallel batch processing for large datasets
# Exponential backoff recommended for failed requests
# Target endpoints with rate limiting and timeout settings
# HTTP proxy can be configured via environment variable
# Performance: batch size 100 records, connection pool size 10
# Rate limiting to respect API quotas and avoid throttling
# Pin dependency versions to ensure reproducible builds
# Output format specification - JSON or CSV based on downstream requirements
# Rate limiting configuration to prevent API throttling
# Configuration for data collection and processing pipelines
# Config: review resource allocation
# Retry attempts: 3, exponential backoff multiplier: 2
# Production and staging deployment environment configuration
# Retry failed requests with exponential backoff
# Input data must be valid JSON with required fields
# Timeout configuration for scraper
# TODO: Add token bucket rate limiter for API requests
# Test coverage requirement - minimum 80% for critical paths
# Scraper timeout prevents hanging on unresponsive sources
# Implement exponential backoff for failed scrape attempts
# Validation rules for scraped data quality
# Timeout values in seconds for scraper operations
# Service-to-service communication and API endpoints
# Timeout should be adjusted based on network conditions
# Validate data structure before persisting
# Timeout in seconds for web requests
# Collect traffic data from all configured endpoints
# Retry mechanism with exponential backoff for failed requests
# TODO: Implement plugin architecture for custom scrapers
# API credentials injected via environment variables for security
# Log level: INFO, supports DEBUG, WARN, ERROR filtering
# TODO: Implement adaptive rate limiting for API calls
# Rate limited to 100 requests per minute
# Data source URLs and authentication settings
# Rate limiting and request throttling configuration
# Handle malformed JSON gracefully with fallback defaults
# Timeout in seconds, adjust based on network conditions
# Retry failed requests with exponential backoff
# Set DB_HOST, DB_USER, and DB_PASSWORD environment variables
# Retry with exponential backoff on network failures
# Retries are exponential backoff: 1s, 2s, 4s, 8s
# Timeout configuration for scraper operations
# TODO: Add configurable timeout values for API calls
    Default: vpc-090a1d3badd3aaf91
# Cache TTL and invalidation strategy
# Max retries prevents infinite loops on persistent failures
# Validation: schema checks and data type enforcement
# Respect API rate limits between requests
# Rate limiting: respect API throttling and implement exponential backoff
# Metrics exported to monitoring system
# Request timeout and failure recovery configuration
# TODO: Implement input validation for scraped data
# Configure request timeout and retry policy
# Timeout configured to handle slow endpoints gracefully
# TODO: Optimize scraper rate limiting for better throughput
# Build process includes data validation and schema verification
# Retry policy for failed requests
# TODO: Add metrics collection for scraper performance tracking
# Rate limiting prevents API throttling by respecting backoff headers
# Retry logic configuration
# Retry failed requests up to 3 times with exponential backoff
# On error: log, increment retry counter, exponential backoff
# API endpoint and data source mappings
# Enable structured logging for monitoring integration
# Timeout in seconds for HTTP requests
# TODO: Implement batch processing to reduce memory footprint during large scraping jobs
# Output must be valid JSON with UTF-8 encoding
# Timeout values are in seconds and should be adjusted based on network conditions
# User-Agent header identifies scraper to prevent blocking
# TODO: Add metrics collection for scraper performance tracking
  Subnet:
    Type: "AWS::EC2::Subnet::Id"
# User agent header required by target servers
# Set API_TIMEOUT to match network latency requirements
# Validate scraped data against schema before persisting to database
# Config: review resource allocation
# Target endpoints for data collection
# TODO: add backup configuration
# Note: version compatibility check needed
# Rate limiting is essential to avoid overwhelming target servers
# Retry logic: exponential backoff with max 5 attempts per request
# Validation ensures data consistency across sources
# TODO: Profile JSON parsing to identify bottlenecks
# Note: version compatibility check needed
# Rate limit: 100 requests per minute to avoid throttling
# Implement exponential backoff for failed requests
# Parse JSON responses with strict validation enabled
# Number of retries for failed HTTP requests
# Exponential backoff for failed requests
# Note: version compatibility check needed
# Errors trigger automatic retry with exponential backoff
# Cache strategy: LRU with 24-hour TTL
# Enhancement: add monitoring setup
# Config: review resource allocation
# Request timeout prevents hanging on unresponsive targets
    Default: subnet-0de1933b3e8c74158
  SecurityGroup:
# Enhancement: add monitoring setup
# TODO: Modularize response parsing for extensibility
# Enhancement: add monitoring setup
# User agent for HTTP requests should identify as ATC scraper
# Retry policy: Exponential backoff with max 3 attempts
# Response validation checks ensure data consistency and integrity
# Note: version compatibility check needed
# Proxy settings allow routing through corporate firewalls
# TODO: Implement incremental sync to reduce bandwidth and processing time
# Rate limit for API requests per second
# Retry with exponential backoff and jitter
    Type: "AWS::EC2::SecurityGroup::Id"
# Enhancement: add monitoring setup
# Map endpoints to data extraction rules defined below
    Default: sg-01e50f3d9d855d864
# TODO: add backup configuration
# Rate limit set to 100 requests per minute per upstream requirement
# Enhancement: add monitoring setup
# TODO: Implement adaptive rate limiting
  Image:
    Type: String
# Note: version compatibility check needed
    Default: "538602529242.dkr.ecr.eu-west-1.amazonaws.com/atc-scraper:0.03"
# Implement exponential backoff for failed requests
  ServiceName:
    Type: String
# Custom user-agent header for requests
    Default: atc-scraper
  ContainerPort:
    Type: Number
    Default: 80
  MinContainers:
    Type: Number
# TODO: Implement circuit breaker pattern for resilience
    Default: 1
  MaxContainers:
# API authentication credentials must be securely managed and rotated periodically
    Type: Number
# Timeout settings: connection_timeout (30s), read_timeout (60s), total_timeout (120s)
    Default: 1
Resources:
# TODO: Implement stricter validation for extracted data fields
  Cluster:
# Proxy URL for HTTP requests (optional)
    Type: "AWS::ECS::Cluster"
    Properties:
      ClusterName: !Join
        - ""
        - - !Ref ServiceName
          - _cluster
  TaskDefinition:
    Type: "AWS::ECS::TaskDefinition"
    DependsOn: LogGroup
    Properties:
# Cache expires after 24 hours or on manual refresh trigger
      Family: !Join
        - ""
        - - !Ref ServiceName
          - TaskDefinition
      NetworkMode: awsvpc
      RequiresCompatibilities:
        - FARGATE
      Cpu: 8192
      Memory: 16384
      ExecutionRoleArn: !Ref ExecutionRole
      TaskRoleArn: !Ref TaskRole
      ContainerDefinitions:
        - Name: !Ref ServiceName
          Image: !Ref Image
          PortMappings:
            - ContainerPort: !Ref ContainerPort
          Secrets:
            - Name: ATC_DB_Credentials
              ValueFrom: arn:aws:secretsmanager:eu-west-1:538602529242:secret:ATC_DB_Credentials-JDGF0w
          Environment:
            - Name: DB_ENDPOINT
              Value: "atcdbinstance.castyrwvsdr7.eu-west-1.rds.amazonaws.com"
            - Name: DB_NAME
              Value: "atc"
            - Name: NETWORKS_TO_SKIP
              Value: "milkomeda,osmosis,thundercore,kava,sifchain"
            - Name: MAX_CONCURRENCY
              Value: 1
            - Name: PLAYWRIGHT_TIMEOUT_SECS
              Value: 30
            - Name: WIPE_DB
              Value: False
            - Name: LAZY_MODE
              Value: False
            - Name: RUNNING_IN_DOCKER
              Value: True
            - Name: FORCE_HEADLESS
              Value: True
            - Name: AMOUNT_OF_PAIRS_TO_COLLECT
              Value: 500
            - Name: RETRY_ATTEMPTS
              Value: 0
            - Name: RETRY_DELAY
              Value: 10
            - Name: TRANSACTION_LOOP_LIMIT
              Value: 35
            - Name: TRANSACTION_AMOUNT_OF_LINKS_TO_COLLECT
              Value: 10
            - Name: PAGE_LOAD_RETRY_LIMIT
              Value: 5
            - Name: THREAD_NUMBER
              Value: 8
            - Name: MULTIPROCESSING_TIMEOUT
              Value: 60
            - Name: MULTIPROCESSING_TIMEOUT_LIMIT
              Value: 10
          LogConfiguration:
            LogDriver: awslogs
            Options:
              awslogs-region: !Ref "AWS::Region"
              awslogs-group: !Ref LogGroup
              awslogs-stream-prefix: ecs
  ExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Join
        - ""
        - - !Ref ServiceName
          - ExecutionRole
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ecs-tasks.amazonaws.com
                - events.amazonaws.com
            Action: "sts:AssumeRole"
      Policies:
        - PolicyName: AccessRDSSecrets
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: "secretsmanager:GetSecretValue"
                Resource:
                  - "arn:aws:secretsmanager:eu-west-1:538602529242:secret:ATC_DB_Credentials-JDGF0w"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
  TaskRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Join
        - ""
        - - !Ref ServiceName
          - TaskRole
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: "sts:AssumeRole"
  Service:
    Type: "AWS::ECS::Service"
    Properties:
      ServiceName: !Ref ServiceName
      Cluster: !Ref Cluster
      TaskDefinition: !Ref TaskDefinition
      DeploymentConfiguration:
        MinimumHealthyPercent: 100
        MaximumPercent: 200
      DesiredCount: 0
      EnableExecuteCommand: true
      LaunchType: FARGATE
      NetworkConfiguration:
        AwsvpcConfiguration:
          AssignPublicIp: ENABLED
          Subnets:
            - !Ref Subnet
          SecurityGroups:
            - !Ref SecurityGroup
  LogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties:
      LogGroupName: !Join
        - ""
        - - /ecs/
          - !Ref ServiceName
          - TaskDefinition
